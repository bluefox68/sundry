1.介绍
在这一课，我们将介绍5个非常有效的机器学习算法用于回归任务。它们每一个也都和分类相当。

好了，现你在开始介绍这个5个算法。我们的目标是解释一些基本的概念（例如：正则化、聚类、自动化属性选择），而不是给你一个长长的算法列表。这些基本概念将会使你懂得为什么一些算法运行效果比其他的效果要好。

2.线性回归的缺点

为了介绍一些高级算法的来龙去脉，让我们从讨论基本的线性回归开始。线性回归模型是非常常见的，但是有严重的缺点。

简单的线性回归模型适合于区间连续的情况（技术上依赖于大量的相同属性的值）。在实际中，它们很少完美的执行。对于大多数机器学习问题，我们实际上推荐忽略它们。

它们的主要优势是它们容易解释和理解。然而，我们的目标是建立一个可以准确预测的模型。

在这一点上，简单的线性回归有两个主要的缺点：
  对于许多输入的属性，它易于过度拟合。
  它不能表示非线性关系。
让我们看看我们如何定位第一个缺点。

3.正则化
这是提高模型的有效性的第一个手段。在许多机器语言课程中，它被认为是高级的，但是它是很容易理解和实现的。

线性模型的第一个缺点是对于许多输入的属性，它易于过度拟合。让我们看一个极端的例子解释发生了什么：
  在你的训练数据中，我们假设有100个观察对象。假设我们也有100个属性。如果你对于那100个属性用线性回归模型，你可以完全记住这个训练数据。每个系数将简单的记住这个观察对象。这个模型在训练数据中将是完全正确的，但是在隐藏的数据中，它就很难得到想要的结果。因为它不是学习了这个真正的模式，而是仅仅记住了训练数据中的噪音。正则化是一个通过人工调整模型系数来阻止过度拟合的技术。
  
  可以降低这个很大的系数（通过调节它们）
  也可以完全移除这个属性（通过设置他们的属性为0）。
  这个调整的力度是可以调节的 
4.正则化后的回归
有三种常见正则化线性回归算法。它们代表最小的绝对收缩和选择操作符。

Lasso

Lasso回归调整了系数的绝对大小。事实上，导致系数可以被精确控制到0.
再次，Lasso回归提出了自动化属性选择因为它可以完全的移除一些特征。
记住，这个调整的力度应该是可调整的。
大力度的调整会导致很多系数趋向于0。

Ridge

Ridge代表用不正确的方法吃葡萄柚（像小孩...他就是ridage）
Ridge 回归调整系数的平方大小。事实上，导致系数变的更小，但是它不会让它们趋向于0。
换句话说，Ridge提供了属性收缩。再次强调下，控制的力度应该是可调整的。
强力的调整会导致系数趋近于0.

5.Elastic-Net

Elastic-Net是介于Lasso 和 Ridge之间的折中方案。
Elastic-Net是绝对大小和平方大小都进行调整。这两个调整类型的比例应该是可调整的。
整个力度也应该是可调整的。
好了，没有更好的控制方法了。它事实上依赖于数据集和问题。我们推荐尝试不同的用一个调整范围作为调整过程的一部分的算法。在明天的课程中，我们再详细讲解。

